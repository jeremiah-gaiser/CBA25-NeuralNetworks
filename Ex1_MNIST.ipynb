{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "wEJ2dyBkEzNf",
        "ovYLKmchFPOh",
        "c5utdogmGi0G",
        "fnEzPmFgH90k",
        "wKsKkesXMqzM",
        "kvSXI_tvOKFi",
        "kfoELj-WONTu",
        "ZoOTqHBrOms0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "Load pytorch libraries and download MNIST data"
      ],
      "metadata": {
        "id": "wEJ2dyBkEzNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "Wh3m05dV7-gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "# MNIST Dataset (Train)\n",
        "train_dataset = datasets.MNIST(root='./data',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "# MNIST Dataset (Test)\n",
        "test_dataset = datasets.MNIST(root='./data',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Train)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "# Data Loader (Test)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "V3mFgfh98VIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cracking open the data\n",
        "Let's see what's inside"
      ],
      "metadata": {
        "id": "ovYLKmchFPOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    print(labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "dxDg1Hvq-yUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a grid of images with the corresponding labels\n",
        "def grid_show(img_list, labels, w=3, h=3):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    count = w*h\n",
        "\n",
        "    for i in range(count):\n",
        "        plt.subplot(h, w, i + 1)\n",
        "        plt.imshow(img_list[i][0])\n",
        "        plt.axis('off')\n",
        "        plt.title(labels[i].item())\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "grid_show(images, labels)"
      ],
      "metadata": {
        "id": "c_cnYzbfAlnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing our model\n",
        "**TODO #1**: identify what is missing in the forward layer.\n",
        "\n",
        "Evaluate the model before and after fixing this. How does this affect the performance?"
      ],
      "metadata": {
        "id": "c5utdogmGi0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)  # Input layer to first hidden layer\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim) # First hidden layer to second hidden layer\n",
        "        self.layer3 = nn.Linear(hidden_dim, output_dim) # Second hidden layer to output layer (num classes in MNIST)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)  # Flatten the image\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        # Wait, what are we missing here?\n",
        "        return x"
      ],
      "metadata": {
        "id": "yo1d2hO5HxnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training/evaluating the model\n",
        "**TODO #2:** Fill in the correct hyperparameters for the model.\n",
        "\n",
        "(One of these hyperparameters does not have a \"correct\" answer)"
      ],
      "metadata": {
        "id": "fnEzPmFgH90k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = 28*28\n",
        "hidden_dim = 256\n",
        "output_dim = 10\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize the model\n",
        "model = MLP(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "V78JXQ_hBxUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "test_loss = nn.CrossEntropyLoss(reduction='none')   # 1 loss per sample\n",
        "model.eval()\n",
        "test_samples, test_losses = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
        "    for images, labels in test_loader:\n",
        "        logits   = model(images)\n",
        "        batch_ls = test_loss(logits, labels)\n",
        "\n",
        "        test_samples.append(images.cpu())\n",
        "        test_losses.append(batch_ls.cpu())\n",
        "\n",
        "test_samples = torch.cat(test_samples)\n",
        "test_losses  = torch.cat(test_losses)\n",
        "\n",
        "sorted_test_samples = test_samples[test_losses.argsort(descending=True)]\n",
        "sorted_test_losses = test_losses[test_losses.argsort(descending=True)]"
      ],
      "metadata": {
        "id": "EYyko4c8MFVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basking in success, meditating on failure\n",
        "**TODO #3:** Use a previously defined function along with `sorted_test_samples` and `sorted_test_losses` to visualize the test samples on which our model performed *very poorly*, and test samples on which our model performed *very well*.\n",
        "\n",
        "**HINT:** *look up pytorch tensor indexing*"
      ],
      "metadata": {
        "id": "wKsKkesXMqzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see some examples."
      ],
      "metadata": {
        "id": "uPk2yUKkD00P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freestyle\n",
        "Almost every update you've made to the code is just one of innumerable ways to configure a neural network. Go mess with the model, and see how your changes affect the model performance. A few lines of inquiry for inspiration:\n",
        "\n",
        "\n",
        "*   How skinny (hidden layer dimension) can you make the network before it starts to fail?\n",
        "*   What happens when you make it super wide?\n",
        "*   What about super deep (more layers)?\n",
        "*   Adjust the hyperparameters: batch size, learning rate, etc. How does this affect the training/testing?\n",
        "*   Can you change the loss function, perhaps to MSELoss? What other changes need to be made for this to work?\n",
        "\n",
        "Go nuts!\n"
      ],
      "metadata": {
        "id": "7JDoZz2wRDOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructor's manual\n",
        "Solutions to all TODOs may be found below."
      ],
      "metadata": {
        "id": "AwKb-UQxHBQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO #1"
      ],
      "metadata": {
        "id": "kvSXI_tvOKFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default model is completely linear, i.e. it has no activation function! The model below implements a proper non-linear operation after each hidden layer."
      ],
      "metadata": {
        "id": "aKefWAOuOx3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)  # Input layer to first hidden layer\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim) # First hidden layer to second hidden layer\n",
        "        self.layer3 = nn.Linear(hidden_dim, output_dim) # Second hidden layer to output layer (num classes in MNIST)\n",
        "\n",
        "        self.act = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)  # Flatten the image\n",
        "        x = self.act(self.layer1(x))\n",
        "        x = self.act(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "QOQpBKYPG3W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO #2"
      ],
      "metadata": {
        "id": "kfoELj-WONTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = 28*28 # Number of pixels in a 28*28 MNIST image\n",
        "output_dim = 10   # Number of classes (labels) in the MNIST dataset\n",
        "hidden_dim = 256  # Whatever the heck you want"
      ],
      "metadata": {
        "id": "2wcre9rROOPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO #3"
      ],
      "metadata": {
        "id": "ZoOTqHBrOms0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `sorted_test_samples` and `sorted_test_losses` are sorted from highest (worst) to lowest (best) loss value.\n",
        "# Therefore, using these tensors as direct input to `grid_show` will show us the worst scoring examples.\n",
        "grid_show(sorted_test_samples, sorted_test_losses)"
      ],
      "metadata": {
        "id": "4EgnliIfOrOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to show examples from anywhere in the tensor, use the proper indexing\n",
        "# Where tensor[n] retrieves the nth item, tensor[n:] (with a colon) slices the tensor from the nth item to the last\n",
        "idx = 500\n",
        "grid_show(sorted_test_samples[idx:], sorted_test_losses[idx:])"
      ],
      "metadata": {
        "id": "C1Z8fgWoPkDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a negative index -n is like saying 'nth from last'\n",
        "# So `sorted_test_samples[-16:]` gives us the last 16 (16 best scoring) items from our tensor.\n",
        "grid_show(sorted_test_samples[-16:], sorted_test_losses[-16:])"
      ],
      "metadata": {
        "id": "rCpQYdTfP8D2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}