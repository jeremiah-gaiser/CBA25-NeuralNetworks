{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0zivH3dLry7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Prerequesites**: Download uniprot data and import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XoafbphKAjkC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-18 01:05:05--  https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz\n",
      "Resolving ftp.uniprot.org (ftp.uniprot.org)... 128.175.240.195\n",
      "Connecting to ftp.uniprot.org (ftp.uniprot.org)|128.175.240.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 682638053 (651M) [application/x-gzip]\n",
      "Saving to: ‘uniprot_sprot.dat.gz’\n",
      "\n",
      "uniprot_sprot.dat.g 100%[===================>] 651.01M   108MB/s    in 6.2s    \n",
      "\n",
      "2025-06-18 01:05:11 (105 MB/s) - ‘uniprot_sprot.dat.gz’ saved [682638053/682638053]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz\n",
    "# !gunzip uniprot_sprot.dat.gz\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see what we're working with.\n",
    "\n",
    "with open('uniprot_sprot.dat') as f_in:\n",
    "    for l_i, line in enumerate(f_in):\n",
    "        print(line.strip())\n",
    "\n",
    "        if l_i > 500:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEF2uJmwNzJl",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Specify UniprotEntry class\n",
    "Stores information about uniprot entry sequence and features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7Eg93s2t-63H"
   },
   "outputs": [],
   "source": [
    "def feature_is_header(line_data):\n",
    "    c1_check = line_data[0] == 'FT'\n",
    "    c2_check = re.match(r\"[A-Z]+\", line_data[1])\n",
    "    c3_check = re.match(r\"<?\\d+(?:\\.\\.\\>?d+)?\", line_data[2])\n",
    "    return c1_check and c2_check and c3_check\n",
    "\n",
    "def get_pos(pos_data):\n",
    "    pos_data = pos_data.replace('<', '')\n",
    "    pos_data = pos_data.replace('>', '')\n",
    "\n",
    "    try:\n",
    "        pos_data = [int(x)-1 for x in pos_data.split('..')]\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    if len(pos_data) == 1:\n",
    "        pos_data.append(pos_data[0]+1)\n",
    "\n",
    "    return pos_data\n",
    "\n",
    "def get_subfeature(feature_string):\n",
    "    feature_search = re.search(r'^\\/(.+)=\"(.+)\"', feature_string)\n",
    "    if feature_search:\n",
    "        return feature_search.groups()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "class UniprotEntry:\n",
    "    def __init__(self, entry_ID):\n",
    "        self.entry_ID = entry_ID\n",
    "        self.features = []\n",
    "        self.sequence = ''\n",
    "\n",
    "    def handle_FT(self, line):\n",
    "        line_data = (line[:2], line[2:21].strip(), line[21:].strip())\n",
    "\n",
    "        if feature_is_header(line_data):\n",
    "            if len(self.features) > 0:\n",
    "                if self.features[-1] == 'ERR':\n",
    "                    self.features.pop()\n",
    "\n",
    "            feature_pos = get_pos(line_data[2])\n",
    "            if feature_pos:\n",
    "                new_feature = [line_data[1], feature_pos, {}]\n",
    "                self.features.append(new_feature)\n",
    "            else:\n",
    "                self.features.append('ERR')\n",
    "        else:\n",
    "            if self.features[-1] == 'ERR':\n",
    "                return\n",
    "\n",
    "            subfeature = get_subfeature(line_data[2])\n",
    "\n",
    "            if subfeature:\n",
    "                if subfeature[0] not in ['evidence', 'id']:\n",
    "                    self.features[-1][-1][subfeature[0]] = subfeature[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGmyydJlOuYS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Process Uniprot Data\n",
    "Iterate through `uniprot_sprot.dat` and collect all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l6ZvK6Z6q5Ay"
   },
   "outputs": [],
   "source": [
    "with open('uniprot_sprot.dat') as uniprot_in:\n",
    "    uniprot_data = []\n",
    "\n",
    "    for l_i, line in enumerate(uniprot_in):\n",
    "        # Uncomment for demo!\n",
    "        # if len(uniprot_data) > 100:\n",
    "        #     break\n",
    "            \n",
    "        c1 = line[:2]\n",
    "\n",
    "        if c1 == 'ID':\n",
    "            if len(uniprot_data) > 0:\n",
    "                if uniprot_data[-1].features[-1] == 'ERR':\n",
    "                    uniprot_data[-1].features.pop()\n",
    "\n",
    "            entry_ID = re.split(r'\\s+', line)[1]\n",
    "            uniprot_data.append(UniprotEntry(entry_ID))\n",
    "\n",
    "        if c1 == 'FT':\n",
    "            uniprot_data[-1].handle_FT(line)\n",
    "\n",
    "        if c1 == '  ':\n",
    "            seq_data = re.split(r'\\s+', line)\n",
    "            uniprot_data[-1].sequence += ''.join(seq_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUOjcSGLPOzm"
   },
   "outputs": [],
   "source": [
    "# Save or load uniprot_data.pkl\n",
    "#\n",
    "# with open('uniprot_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(uniprot_data, f)\n",
    "\n",
    "# with open('uniprot_data.pkl', 'rb') as f:\n",
    "#     uniprot_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXLWbve9PRD3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check!\n",
    "# Check our uniprot data entries behave as we expect\n",
    "for e_i, entry in enumerate(uniprot_data):\n",
    "    print(entry.entry_ID)\n",
    "    print(entry.sequence)\n",
    "    print(entry.features)\n",
    "    print('-----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPl7BmeRQi_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filter and Format\n",
    "Identify the samples we are interested in and convert sequence data and labels to one-hot values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LOm0Ax5FwSvd"
   },
   "outputs": [],
   "source": [
    "feature_labels = [\n",
    " 'DNA_BIND',\n",
    " 'REGION-Disordered',\n",
    " 'TOPO_DOM-Cytoplasmic',\n",
    " 'TOPO_DOM-Extracellular',\n",
    " 'TOPO_DOM-Lumenal',\n",
    " 'TRANSIT-Mitochondrion',\n",
    " 'TRANSMEM-Helical',\n",
    " 'ZN_FING'\n",
    "]\n",
    "\n",
    "seq_vocab = ['-','A','B','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "\n",
    "def get_label_index(feature):\n",
    "    for k,v in feature[-1].items():\n",
    "        feature_string = f\"{feature[0]}-{v}\"\n",
    "        for idx, label in enumerate(feature_labels):\n",
    "            if label in feature_string:\n",
    "                return idx\n",
    "    return None\n",
    "\n",
    "def get_samples(entry, feature_labels, max_l=20):\n",
    "    samples = []\n",
    "    for feature in entry.features:\n",
    "        label_idx = get_label_index(feature)\n",
    "\n",
    "        if label_idx is not None:\n",
    "            seq_slice = entry.sequence[slice(*feature[1])][:max_l]\n",
    "\n",
    "            if len(seq_slice) < max_l:\n",
    "                seq_slice += '-'*(max_l-len(seq_slice))\n",
    "\n",
    "            samples.append((seq_slice, label_idx))\n",
    "\n",
    "    return samples\n",
    "\n",
    "def generate_onehot(size, idx):\n",
    "    onehot = [0 for _ in range(size)]\n",
    "    onehot[idx] = 1\n",
    "    return onehot\n",
    "\n",
    "def seq_to_onehot(seq, vocab):\n",
    "    seq_onehot = []\n",
    "    for c in seq:\n",
    "        symbol_onehot = generate_onehot(len(vocab), vocab.index(c))\n",
    "        seq_onehot += symbol_onehot\n",
    "    return seq_onehot\n",
    "\n",
    "sample_seqs = []\n",
    "sample_labels = []\n",
    "\n",
    "for ei, entry in enumerate(uniprot_data):\n",
    "    samples = get_samples(entry, feature_labels)\n",
    "\n",
    "    if samples is not None:\n",
    "        for seq, label in samples:\n",
    "            sample_seqs.append(seq_to_onehot(seq, seq_vocab))\n",
    "            sample_labels.append(generate_onehot(len(feature_labels), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0EwtSzvXRPjZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "500 25\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for seq, label in zip(sample_seqs, sample_labels):\n",
    "    print(seq)\n",
    "    print(len(seq), len(seq_vocab))\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MYqP4rNRFluf"
   },
   "outputs": [],
   "source": [
    "sample_seqs = torch.tensor(sample_seqs)\n",
    "sample_labels = torch.tensor(sample_labels)\n",
    "\n",
    "uniprot_corpus = (sample_seqs, sample_labels, seq_vocab, feature_labels)\n",
    "# torch.save((sample_seqs, sample_labels, seq_vocab, feature_labels), '/content/drive/My Drive/uniprot_corpus.pt')\n",
    "#uniprot_corpus = torch.load('/content/drive/My Drive/uniprot_corpus.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rzVO2-knZqOT"
   },
   "outputs": [],
   "source": [
    "fun_hot_seqs = sample_seqs.reshape((-1, 20, len(seq_vocab))).argmax(2)\n",
    "fun_hot_labels = sample_labels.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-acvqiRaXr-y"
   },
   "outputs": [],
   "source": [
    "for seq_index, label_index in zip(fun_hot_seqs, fun_hot_labels):\n",
    "    print(''.join([seq_vocab[i] for i in seq_index]), feature_labels[label_index])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dataloader\n",
    "Specify a class which partitions the data into train-test splits and retrieves randomized batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_corpus = torch.load('uniprot_corpus.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "h5JBBtIrMnCD"
   },
   "outputs": [],
   "source": [
    "class UniprotLoader:\n",
    "    def __init__(self, uniprot_corpus, batch_size=64):\n",
    "        self.sequences = uniprot_corpus[0]\n",
    "        self.labels = uniprot_corpus[1]\n",
    "        self.sequence_vocab = uniprot_corpus[2]\n",
    "        self.label_vocab = uniprot_corpus[3]\n",
    "\n",
    "        self.seq_length = self.sequences.shape[1]\n",
    "        self.label_length = self.labels.shape[1]\n",
    "\n",
    "        self.train_index, self.test_index = self.get_train_test_splits()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_iterator = self.new_batch_iterator(self.train_index)\n",
    "        self.test_iterator = self.new_batch_iterator(self.test_index)\n",
    "\n",
    "        self.train_batch_count = len(self.train_index) // self.batch_size\n",
    "        self.test_batch_count = len(self.test_index) // self.batch_size\n",
    "\n",
    "    def new_batch_iterator(self, index):\n",
    "        random_index = index[torch.randperm(len(index))]\n",
    "\n",
    "        for i in range(0, len(random_index), self.batch_size):\n",
    "            yield random_index[i : i + self.batch_size]\n",
    "\n",
    "    def get_train_test_splits(self, r=0.2):\n",
    "        train_index = []\n",
    "        test_index = []\n",
    "\n",
    "        for l_idx in range(len(self.label_vocab)):\n",
    "            class_index = torch.where(self.labels[:, l_idx]==1)[0]\n",
    "            class_size = len(class_index)\n",
    "            class_index_random = class_index[torch.randperm(class_size)]\n",
    "\n",
    "            test_cutoff = int(class_size*r)\n",
    "\n",
    "            test_index.append(class_index_random[:test_cutoff])\n",
    "            train_index.append(class_index_random[test_cutoff:])\n",
    "\n",
    "        return(torch.hstack(train_index).sort()[0], torch.hstack(test_index).sort()[0])\n",
    "\n",
    "    def get_batch(self, dataset='train'):\n",
    "        iterator = self.train_iterator if dataset == 'train' else self.test_iterator\n",
    "        random_index = next(iterator, None)\n",
    "\n",
    "        if random_index is None:\n",
    "            if dataset == 'train':\n",
    "                iterator = self.new_batch_iterator(self.train_index)\n",
    "                self.train_iterator = iterator\n",
    "            else:\n",
    "                iterator = self.new_batch_iterator(self.test_index)\n",
    "                self.test_iterator = iterator\n",
    "\n",
    "            random_index = next(self.train_iterator, None)\n",
    "\n",
    "        return self.sequences[random_index].float(), self.labels[random_index].float()\n",
    "\n",
    "\n",
    "uniprot_loader = UniprotLoader(uniprot_corpus, batch_size=2048)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your model\n",
    "Construct your sequence classifier and test it in battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-T9XYaYcWbB3"
   },
   "outputs": [],
   "source": [
    "class SequenceClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.input_layer = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = self.generate_hidden_layers(4, torch.nn.ReLU())\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def generate_hidden_layers(self, layer_count, activation):\n",
    "        layers = []\n",
    "\n",
    "        for _ in range(layer_count):\n",
    "            layers.append(torch.nn.Linear(in_features=self.hidden_size,\n",
    "                                          out_features=self.hidden_size))\n",
    "            layers.append(activation)\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cvLA6W7ar14f"
   },
   "outputs": [],
   "source": [
    "sequence_classifier = SequenceClassifier(uniprot_loader.seq_length, 1024, len(uniprot_loader.label_vocab)).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(sequence_classifier.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----Epoch [1/100], Batch [100/298]-----|\n",
      "| Train Loss: 0.0152\n",
      "| Per-label test accuracy: 0.92 | 0.98 | 0.80 | 0.76 | 0.52 | 0.79 | 1.00 | 0.98\n",
      "|----------------------------------------|\n",
      "\n",
      "|-----Epoch [1/100], Batch [200/298]-----|\n",
      "| Train Loss: 0.0145\n",
      "| Per-label test accuracy: 0.92 | 0.97 | 0.80 | 0.76 | 0.51 | 0.83 | 1.00 | 0.98\n",
      "|----------------------------------------|\n",
      "\n",
      "|-----Epoch [2/100], Batch [100/298]-----|\n",
      "| Train Loss: 0.0166\n",
      "| Per-label test accuracy: 0.92 | 0.97 | 0.81 | 0.75 | 0.53 | 0.83 | 1.00 | 0.98\n",
      "|----------------------------------------|\n",
      "\n",
      "|-----Epoch [2/100], Batch [200/298]-----|\n",
      "| Train Loss: 0.0160\n",
      "| Per-label test accuracy: 0.93 | 0.97 | 0.80 | 0.75 | 0.53 | 0.81 | 1.00 | 0.97\n",
      "|----------------------------------------|\n",
      "\n",
      "|-----Epoch [3/100], Batch [100/298]-----|\n",
      "| Train Loss: 0.0156\n",
      "| Per-label test accuracy: 0.92 | 0.97 | 0.80 | 0.76 | 0.53 | 0.79 | 1.00 | 0.98\n",
      "|----------------------------------------|\n",
      "\n",
      "|-----Epoch [3/100], Batch [200/298]-----|\n",
      "| Train Loss: 0.0150\n",
      "| Per-label test accuracy: 0.92 | 0.98 | 0.81 | 0.73 | 0.52 | 0.83 | 1.00 | 0.98\n",
      "|----------------------------------------|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, loss_function, optimizer, test_freq=100):\n",
    "    device = next(model.parameters()).device\n",
    "    loss_vals = []\n",
    "\n",
    "    for batch_index in range(dataloader.train_batch_count):\n",
    "        model.train()\n",
    "        batch_samples, batch_labels = dataloader.get_batch()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_samples.to(device))\n",
    "        loss = loss_function(outputs, torch.argmax(batch_labels.to(device), dim=1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_vals.append(loss.item())\n",
    "\n",
    "        if (batch_index + 1) % test_freq == 0: \n",
    "            test_accuracy = test(dataloader, model)\n",
    "            accuracy_report = ' | '.join([f'{x.item():0.2f}' for x in test_accuracy]) \n",
    "            avg_train_loss = sum(loss_vals) / len(loss_vals)\n",
    "            \n",
    "            report_string = [f'|-----Epoch [%d/%d], Batch [{batch_index+1}/{uniprot_loader.train_batch_count}]-----|\\n',\n",
    "                             f'| Train Loss: {avg_train_loss:.4f}\\n',\n",
    "                             f'| Per-label test accuracy: {accuracy_report}\\n']\n",
    "\n",
    "            report_string.append('|' + ''.join(['-' for _ in range(len(report_string[0])-3)]) + '|\\n')\n",
    "            \n",
    "            yield ''.join(report_string)\n",
    "            \n",
    "\n",
    "\n",
    "def test(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        totals = torch.zeros(len(dataloader.label_vocab))\n",
    "        correct = torch.zeros(len(dataloader.label_vocab))\n",
    "    \n",
    "        for batch_index in range(dataloader.test_batch_count):\n",
    "            batch_samples, batch_labels = dataloader.get_batch(dataset='test')\n",
    "    \n",
    "            outputs = model(batch_samples.to(device))\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            true_labels = batch_labels.argmax(dim=1).to(device)\n",
    "            hits = (predictions == true_labels)\n",
    "            correct_label_index, correct_counts = true_labels[hits].cpu().unique(return_counts=True)\n",
    "    \n",
    "            totals += torch.sum(batch_labels, dim=0)\n",
    "            correct[correct_label_index] += correct_counts\n",
    "    \n",
    "        return (correct / totals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----Epoch [1/100], Batch [100/298]-----|\n",
      "Train Loss: 0.0175\n",
      "Per-label test accuracy: 0.925 0.969 0.809 0.720 0.519 0.862 0.996 0.982\n",
      "--------------\n",
      "\n",
      "|-----Epoch [1/100], Batch [200/298]-----|\n",
      "Train Loss: 0.0174\n",
      "Per-label test accuracy: 0.922 0.971 0.810 0.753 0.513 0.797 0.996 0.978\n",
      "--------------\n",
      "\n",
      "|-----Epoch [2/100], Batch [100/298]-----|\n",
      "Train Loss: 0.0153\n",
      "Per-label test accuracy: 0.910 0.972 0.812 0.738 0.531 0.815 0.996 0.979\n",
      "--------------\n",
      "\n",
      "|-----Epoch [2/100], Batch [200/298]-----|\n",
      "Train Loss: 0.0160\n",
      "Per-label test accuracy: 0.937 0.973 0.811 0.733 0.519 0.770 0.997 0.974\n",
      "--------------\n",
      "\n",
      "|-----Epoch [3/100], Batch [100/298]-----|\n",
      "Train Loss: 0.0163\n",
      "Per-label test accuracy: 0.912 0.977 0.797 0.752 0.504 0.848 0.997 0.975\n",
      "--------------\n",
      "\n",
      "|-----Epoch [3/100], Batch [200/298]-----|\n",
      "Train Loss: 0.0162\n",
      "Per-label test accuracy: 0.922 0.967 0.818 0.740 0.508 0.798 0.997 0.977\n",
      "--------------\n",
      "\n",
      "|-----Epoch [4/100], Batch [100/298]-----|\n",
      "Train Loss: 0.0153\n",
      "Per-label test accuracy: 0.921 0.965 0.811 0.736 0.548 0.867 0.997 0.977\n",
      "--------------\n",
      "\n",
      "|-----Epoch [4/100], Batch [200/298]-----|\n",
      "Train Loss: 0.0156\n",
      "Per-label test accuracy: 0.924 0.971 0.815 0.735 0.522 0.861 0.997 0.979\n",
      "--------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m epochs = \u001b[32m100\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniprot_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(dataloader, model, loss_function, optimizer, test_freq)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dataloader.train_batch_count):\n\u001b[32m      6\u001b[39m     model.train()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     batch_samples, batch_labels = \u001b[43mdataloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     optimizer.zero_grad()\n\u001b[32m     11\u001b[39m     outputs = model(batch_samples.to(device))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mUniprotLoader.get_batch\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[38;5;28mself\u001b[39m.test_iterator = iterator\n\u001b[32m     55\u001b[39m     random_index = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_iterator, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.labels[random_index].float()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(100):\n",
    "    for output in train(uniprot_loader, sequence_classifier, loss_function, optimizer):\n",
    "        print(output % (epoch+1, epochs))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "N0zivH3dLry7",
    "lEF2uJmwNzJl"
   ],
   "gpuType": "L4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
