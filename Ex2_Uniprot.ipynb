{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "N0zivH3dLry7",
        "lEF2uJmwNzJl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wtFjVA-cDqzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prerequesites**: Download uniprot data and import python libraries"
      ],
      "metadata": {
        "id": "N0zivH3dLry7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz\n",
        "# !gunzip uniprot_sprot.dat.gz\n",
        "# shutil.copyfile('uniprot_sprot.dat', '/content/drive/My Drive/uniprot_sprat.dat')\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil"
      ],
      "metadata": {
        "id": "XoafbphKAjkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify UniprotEntry class\n",
        "Stores information about uniprot entry sequence and features\n"
      ],
      "metadata": {
        "id": "lEF2uJmwNzJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_is_header(line_data):\n",
        "    c1_check = line_data[0] == 'FT'\n",
        "    c2_check = re.match(r\"[A-Z]+\", line_data[1])\n",
        "    c3_check = re.match(r\"<?\\d+(?:\\.\\.\\>?d+)?\", line_data[2])\n",
        "    return c1_check and c2_check and c3_check\n",
        "\n",
        "def get_pos(pos_data):\n",
        "    pos_data = pos_data.replace('<', '')\n",
        "    pos_data = pos_data.replace('>', '')\n",
        "\n",
        "    try:\n",
        "        pos_data = [int(x)-1 for x in pos_data.split('..')]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "    if len(pos_data) == 1:\n",
        "        pos_data.append(pos_data[0]+1)\n",
        "\n",
        "    return pos_data\n",
        "\n",
        "def get_subfeature(feature_string):\n",
        "    feature_search = re.search(r'^\\/(.+)=\"(.+)\"', feature_string)\n",
        "    if feature_search:\n",
        "        return feature_search.groups()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "class UniprotEntry:\n",
        "    def __init__(self, entry_ID):\n",
        "        self.entry_ID = entry_ID\n",
        "        self.features = []\n",
        "        self.sequence = ''\n",
        "\n",
        "    def handle_FT(self, line):\n",
        "        line_data = (line[:2], line[2:21].strip(), line[21:].strip())\n",
        "\n",
        "        if feature_is_header(line_data):\n",
        "            if len(self.features) > 0:\n",
        "                if self.features[-1] == 'ERR':\n",
        "                    self.features.pop()\n",
        "\n",
        "            feature_pos = get_pos(line_data[2])\n",
        "            if feature_pos:\n",
        "                new_feature = [line_data[1], feature_pos, {}]\n",
        "                self.features.append(new_feature)\n",
        "            else:\n",
        "                self.features.append('ERR')\n",
        "        else:\n",
        "            if self.features[-1] == 'ERR':\n",
        "                return\n",
        "\n",
        "            subfeature = get_subfeature(line_data[2])\n",
        "\n",
        "            if subfeature:\n",
        "                if subfeature[0] not in ['evidence', 'id']:\n",
        "                    self.features[-1][-1][subfeature[0]] = subfeature[1]\n"
      ],
      "metadata": {
        "id": "7Eg93s2t-63H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Uniprot Data\n",
        "Iterate through `uniprot_sprot.dat` and collect all features"
      ],
      "metadata": {
        "id": "NGmyydJlOuYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/uniprot_sprat.dat') as uniprot_in:\n",
        "    uniprot_data = []\n",
        "\n",
        "    for l_i, line in enumerate(uniprot_in):\n",
        "        c1 = line[:2]\n",
        "\n",
        "        if c1 == 'ID':\n",
        "            if len(uniprot_data) > 0:\n",
        "                if uniprot_data[-1].features[-1] == 'ERR':\n",
        "                    uniprot_data[-1].features.pop()\n",
        "\n",
        "            entry_ID = re.split(r'\\s+', line)[1]\n",
        "            uniprot_data.append(UniprotEntry(entry_ID))\n",
        "\n",
        "        if c1 == 'FT':\n",
        "            uniprot_data[-1].handle_FT(line)\n",
        "\n",
        "        if c1 == '  ':\n",
        "            seq_data = re.split(r'\\s+', line)\n",
        "            uniprot_data[-1].sequence += ''.join(seq_data)\n",
        "            # uniprot_data[-1].sequence += line.strip()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l6ZvK6Z6q5Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save or load uniprot_data.pkl\n",
        "#\n",
        "# with open('/content/drive/My Drive/uniprot_data.pkl', 'wb') as f:\n",
        "#     pickle.dump(uniprot_data, f)\n",
        "\n",
        "# with open('/content/drive/My Drive/uniprot_data.pkl', 'rb') as f:\n",
        "#     uniprot_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "EUOjcSGLPOzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check!\n",
        "\n",
        "for e_i, entry in enumerate(uniprot_data):\n",
        "    print(entry.entry_ID)\n",
        "    print(entry.sequence)\n",
        "    print(entry.features)\n",
        "    print('-----')\n",
        "\n",
        "    if e_i > 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "LXLWbve9PRD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter and Format\n",
        "Identify the samples we are interested in and convert sequence data and labels to one-hot values."
      ],
      "metadata": {
        "id": "fIPl7BmeRQi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_labels = [\n",
        " 'DNA_BIND',\n",
        " 'REGION-Disordered',\n",
        " 'TOPO_DOM-Cytoplasmic',\n",
        " 'TOPO_DOM-Extracellular',\n",
        " 'TOPO_DOM-Lumenal',\n",
        " 'TRANSIT-Mitochondrion',\n",
        " 'TRANSMEM-Helical',\n",
        " 'ZN_FING'\n",
        "]\n",
        "\n",
        "seq_vocab = ['-','A','B','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
        "\n",
        "def get_label_index(feature):\n",
        "    for k,v in feature[-1].items():\n",
        "        feature_string = f\"{feature[0]}-{v}\"\n",
        "        for idx, label in enumerate(feature_labels):\n",
        "            if label in feature_string:\n",
        "                return idx\n",
        "    return None\n",
        "\n",
        "def get_samples(entry, feature_labels, max_l=20):\n",
        "    samples = []\n",
        "    for feature in entry.features:\n",
        "        label_idx = get_label_index(feature)\n",
        "\n",
        "        if label_idx is not None:\n",
        "            seq_slice = entry.sequence[slice(*feature[1])][:max_l]\n",
        "\n",
        "            if len(seq_slice) < max_l:\n",
        "                seq_slice += '-'*(max_l-len(seq_slice))\n",
        "\n",
        "            samples.append((seq_slice, label_idx))\n",
        "\n",
        "    return samples\n",
        "\n",
        "def generate_onehot(size, idx):\n",
        "    onehot = [0 for _ in range(size)]\n",
        "    onehot[idx] = 1\n",
        "    return onehot\n",
        "\n",
        "def seq_to_onehot(seq, vocab):\n",
        "    seq_onehot = []\n",
        "    for c in seq:\n",
        "        symbol_onehot = generate_onehot(len(vocab), vocab.index(c))\n",
        "        seq_onehot += symbol_onehot\n",
        "    return seq_onehot\n",
        "\n",
        "sample_seqs = []\n",
        "sample_labels = []\n",
        "\n",
        "for ei, entry in enumerate(uniprot_data):\n",
        "    samples = get_samples(entry, feature_labels)\n",
        "\n",
        "    if samples is not None:\n",
        "        for seq, label in samples:\n",
        "            sample_seqs.append(seq_to_onehot(seq, seq_vocab))\n",
        "            sample_labels.append(generate_onehot(len(feature_labels), label))"
      ],
      "metadata": {
        "id": "LOm0Ax5FwSvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq, label in zip(sample_seqs, sample_labels):\n",
        "    print(seq)\n",
        "    print(len(seq), len(seq_vocab))\n",
        "    print(label)\n",
        "    break"
      ],
      "metadata": {
        "id": "0EwtSzvXRPjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_seqs = torch.tensor(sample_seqs)\n",
        "sample_labels = torch.tensor(sample_labels)\n",
        "\n",
        "uniprot_corpus = (sample_seqs, sample_labels, seq_vocab, feature_labels)\n",
        "# torch.save((sample_seqs, sample_labels, seq_vocab, feature_labels), '/content/drive/My Drive/uniprot_corpus.pt')\n",
        "#uniprot_corpus = torch.load('/content/drive/My Drive/uniprot_corpus.pt')"
      ],
      "metadata": {
        "id": "MYqP4rNRFluf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYzmofJsaTSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fun_hot_seqs = sample_seqs.reshape((-1, 20, len(seq_vocab))).argmax(2)\n",
        "fun_hot_labels = sample_labels.argmax(1)"
      ],
      "metadata": {
        "id": "rzVO2-knZqOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r_i, data_row in enumerate(zip(fun_hot_seqs, fun_hot_labels)):\n",
        "    seq_index, label_index = data_row\n",
        "    print(''.join([seq_vocab[i] for i in seq_index]), feature_labels[label_index])\n",
        "\n",
        "    if r_i > 20:\n",
        "        break"
      ],
      "metadata": {
        "id": "-acvqiRaXr-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UniprotLoader:\n",
        "    def __init__(self, uniprot_corpus, batch_size=64):\n",
        "        self.sequences = uniprot_corpus[0]\n",
        "        self.labels = uniprot_corpus[1]\n",
        "        self.sequence_vocab = uniprot_corpus[2]\n",
        "        self.label_vocab = uniprot_corpus[3]\n",
        "\n",
        "        self.seq_length = self.sequences.shape[1]\n",
        "        self.label_length = self.labels.shape[1]\n",
        "\n",
        "        self.train_index, self.test_index = self.get_train_test_splits()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.train_iterator = self.new_batch_iterator(self.train_index)\n",
        "        self.test_iterator = self.new_batch_iterator(self.test_index)\n",
        "\n",
        "        self.train_batch_count = len(self.train_index) // self.batch_size\n",
        "        self.test_batch_count = len(self.test_index) // self.batch_size\n",
        "\n",
        "    def new_batch_iterator(self, index):\n",
        "        random_index = index[torch.randperm(len(index))]\n",
        "\n",
        "        for i in range(0, len(random_index), self.batch_size):\n",
        "            yield random_index[i : i + self.batch_size]\n",
        "\n",
        "    def get_train_test_splits(self, r=0.2):\n",
        "        train_index = []\n",
        "        test_index = []\n",
        "\n",
        "        for l_idx in range(len(self.label_vocab)):\n",
        "            class_index = torch.where(self.labels[:, l_idx]==1)[0]\n",
        "            class_size = len(class_index)\n",
        "            class_index_random = class_index[torch.randperm(class_size)]\n",
        "\n",
        "            test_cutoff = int(class_size*r)\n",
        "\n",
        "            test_index.append(class_index_random[:test_cutoff])\n",
        "            train_index.append(class_index_random[test_cutoff:])\n",
        "\n",
        "        return(torch.hstack(train_index).sort()[0], torch.hstack(test_index).sort()[0])\n",
        "\n",
        "    def get_batch(self, dataset='train'):\n",
        "        iterator = self.train_iterator if dataset == 'train' else self.test_iterator\n",
        "        random_index = next(iterator, None)\n",
        "\n",
        "        if random_index is None:\n",
        "            if dataset == 'train':\n",
        "                iterator = self.new_batch_iterator(self.train_index)\n",
        "                self.train_iterator = iterator\n",
        "            else:\n",
        "                iterator = self.new_batch_iterator(self.test_index)\n",
        "                self.test_iterator = iterator\n",
        "\n",
        "            random_index = next(self.train_iterator, None)\n",
        "\n",
        "        return self.sequences[random_index].float(), self.labels[random_index].float()\n",
        "\n",
        "\n",
        "uniprot_loader = UniprotLoader(uniprot_corpus, batch_size=2048)"
      ],
      "metadata": {
        "id": "h5JBBtIrMnCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SequenceClassifier, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.input_layer = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_layers = self.generate_hidden_layers(4, torch.nn.ReLU())\n",
        "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def generate_hidden_layers(self, layer_count, activation):\n",
        "        layers = []\n",
        "\n",
        "        for _ in range(layer_count):\n",
        "            layers.append(torch.nn.Linear(in_features=self.hidden_size,\n",
        "                                          out_features=self.hidden_size))\n",
        "            layers.append(activation)\n",
        "\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.hidden_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "sequence_classifier = SequenceClassifier(uniprot_loader.seq_length, 1024, len(uniprot_loader.label_vocab)).to(device)\n"
      ],
      "metadata": {
        "id": "-T9XYaYcWbB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(sequence_classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "cvLA6W7ar14f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: pytorch training loop boilerplate\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    sequence_classifier.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index in range(uniprot_loader.train_batch_count):\n",
        "        batch_samples, batch_labels = uniprot_loader.get_batch()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = sequence_classifier(batch_samples.to(device))\n",
        "        loss = loss_function(outputs, torch.argmax(batch_labels.to(device), dim=1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_index + 1) % 100 == 0:    # Print every 10 mini-batches\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_index+1}/{uniprot_loader.train_batch_count}], Loss: {running_loss/10:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sequence_classifier.eval()\n",
        "                totals = torch.zeros(len(uniprot_loader.label_vocab))\n",
        "                correct = torch.zeros(len(uniprot_loader.label_vocab))\n",
        "\n",
        "                for batch_index in range(uniprot_loader.test_batch_count):\n",
        "                    batch_samples, batch_labels = uniprot_loader.get_batch(dataset='test')\n",
        "\n",
        "                    outputs = sequence_classifier(batch_samples.to(device))\n",
        "                    predictions = outputs.argmax(dim=1)\n",
        "                    true_labels = batch_labels.argmax(dim=1).to(device)\n",
        "                    hits = (predictions == true_labels)\n",
        "                    correct_label_index, correct_counts = true_labels[hits].cpu().unique(return_counts=True)\n",
        "\n",
        "                    totals += torch.sum(batch_labels, dim=0)\n",
        "                    correct[correct_label_index] += correct_counts\n",
        "\n",
        "                print(correct / totals)\n",
        "                sequence_classifier.train()\n",
        "\n",
        "\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "DH6q-rWcrS4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}